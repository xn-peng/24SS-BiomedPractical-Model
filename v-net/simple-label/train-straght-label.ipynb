{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64702cbc-c254-4b3a-96e8-7c7e0044361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from model_v2 import CombinedVNetModel\n",
    "\n",
    "\n",
    "from data_loader import load_nifti, resample\n",
    "import logging\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586373b6-a555-4d12-97e4-c9f22580ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "directory = '../../../Aims-Tbi'\n",
    "demographics_file = '../../TestSet_demographics_with_lesion.csv'\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2656d32-1bc2-44da-8040-bf68e1c5f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, directory, demographics_file, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "        self.demographics = pd.read_csv(demographics_file)\n",
    "\n",
    "        self.demographics['Sex'] = self.demographics['Sex'].astype(float)\n",
    "\n",
    "        self.t1_paths = []\n",
    "        self.diag_data = []\n",
    "\n",
    "        available_mris = set(f.replace('_T1.nii.gz', '') for f in os.listdir(directory) if f.endswith('_T1.nii.gz'))\n",
    "\n",
    "        for _, row in self.demographics.iterrows():\n",
    "            rand_id = row['RandID']\n",
    "            if rand_id in available_mris:\n",
    "                mri_file = os.path.join(directory, f\"{rand_id}_T1.nii.gz\")\n",
    "                if os.path.exists(mri_file):\n",
    "                    self.t1_paths.append(mri_file)\n",
    "                    self.diag_data.append(row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t1_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t1_path = self.t1_paths[idx]\n",
    "        \n",
    "        \n",
    "        diag_row = self.diag_data[idx]\n",
    "\n",
    "        mri_img = load_nifti(t1_path)\n",
    "        \n",
    "        mri_img = (mri_img - np.min(mri_img)) / (np.max(mri_img) - np.min(mri_img))\n",
    "\n",
    "        mri_img = resample(mri_img, (256, 256, 256))\n",
    "        \n",
    "        mri_tensor = torch.tensor(mri_img, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        lebel_data = diag_row['Lesion']\n",
    "        label_tensor = torch.tensor(lebel_data, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        manufacturer_encoding = {'Siemens': 1, 'Philips': 2, 'GE': 3}\n",
    "\n",
    "        diag_values = [\n",
    "            float(diag_row['Age']),\n",
    "            float(diag_row['Sex']),\n",
    "            float(diag_row['TSI']),\n",
    "            manufacturer_encoding[diag_row['ScanManufacturer']]\n",
    "        ]\n",
    "\n",
    "        diag_numpy = np.array(diag_values, dtype=np.float32)\n",
    "        diag_tensor = torch.tensor(diag_numpy)\n",
    "\n",
    "        if self.transform:\n",
    "            mri_tensor = self.transform(mri_tensor)\n",
    "            label_tensor = self.transform(label_tensor)\n",
    "\n",
    "        return mri_tensor, label_tensor, diag_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeee5a7-4b1a-46da-b187-7ab9c3649853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory, demographics_file, batch_size):\n",
    "    \n",
    "    \n",
    "    train_dataset_proportion = 0.25\n",
    "    val_dataset_proportion = 0.1\n",
    "    test_dataset_proportion = 0.3\n",
    "\n",
    "    dataset = MRIDataset(directory=directory, demographics_file=demographics_file)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(\"The length of the dataloader:\", len(dataloader));\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    \n",
    "    print(f\"Total dataset size: {total_size}\")\n",
    "\n",
    "    train_size = int(total_size * train_dataset_proportion)\n",
    "    val_size = int(total_size * val_dataset_proportion)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    if val_size == 0 or test_size == 0:\n",
    "        raise ValueError(\"Dataset too small to split according to the specified ratios. Increase dataset size or adjust the ratios.\")\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(f\"Training set size: {len(train_loader)}\")\n",
    "    print(f\"Validation set size: {len(val_loader)}\")\n",
    "    print(f\"Test set size: {len(test_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a70ce8-d547-4789-8d5f-80b826a0dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        logging.info(f\"Loaded checkpoint '{filename}' (epoch {start_epoch})\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        logging.info(f\"No checkpoint found at '{filename}'\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb75dfc-b3da-41da-83c0-9dc01a11d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_values(output, n):\n",
    "    count = torch.sum(output > 0.5).item()\n",
    "    \n",
    "    if count >= n:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4e1f48-e2aa-43ea-8065-cf14c887f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10, device='cpu', checkpoint_file='checkpoint.pth.tar'):\n",
    "    start_epoch = load_checkpoint(checkpoint_file, model, optimizer)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(total=len(train_loader.dataset), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for t1, lesion, extra_features in train_loader:\n",
    "            t1 = t1.to(device)\n",
    "            lesion = lesion.to(device).float()\n",
    "            extra_features = extra_features.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(t1, extra_features)\n",
    "            loss = criterion(outputs, lesion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.update(len(t1))\n",
    "\n",
    "        progress_bar.set_postfix(loss=train_loss/len(train_loader))\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for t1, lesion, extra_features in val_loader:\n",
    "                t1 = t1.to(device)\n",
    "                lesion = lesion.to(device)\n",
    "                extra_features = extra_features.to(device)\n",
    "\n",
    "                outputs = model(t1, extra_features)\n",
    "                loss = criterion(outputs, lesion)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, checkpoint_file)\n",
    "\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    print('Model saved as final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e02a38-8b44-4df2-ae0f-62e2afb1a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory):\n",
    "    raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "if not os.path.exists(demographics_file):\n",
    "    raise FileNotFoundError(f\"Demographics file not found: {demographics_file}\")\n",
    "\n",
    "num_extra_features = 4\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model = CombinedVNetModel(num_extra_features, use_downsampling=True).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "batch_size = 1\n",
    "num_epochs = 5\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(directory, demographics_file, batch_size)\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, device=device, checkpoint_file='checkpoint.pth.tar')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
